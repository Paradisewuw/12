{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.utils import gen_batches\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from typing import *\n",
    "from numpy.linalg import *\n",
    "\n",
    "train_image_file = './mnist/train-images-idx3-ubyte'\n",
    "train_label_file = './mnist/train-labels-idx1-ubyte'\n",
    "test_image_file = './mnist/t10k-images-idx3-ubyte'\n",
    "test_label_file = './mnist/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def decode_image(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(-1, 784)\n",
    "        images = np.array(images, dtype = float)\n",
    "    return images\n",
    "\n",
    "def decode_label(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, n = struct.unpack('>II',f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        labels = np.array(labels, dtype = float)\n",
    "    return labels\n",
    "\n",
    "def load_data():\n",
    "    train_X = decode_image(train_image_file)\n",
    "    train_Y = decode_label(train_label_file)\n",
    "    test_X = decode_image(test_image_file)\n",
    "    test_Y = decode_label(test_label_file)\n",
    "    return (train_X, train_Y, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = load_data()\n",
    "\n",
    "num_train, num_feature = trainX.shape\n",
    "plt.figure(1, figsize=(20,10))\n",
    "for i in range(8):\n",
    "    idx = np.random.choice(range(num_train))\n",
    "    plt.subplot(int('24'+str(i+1)))\n",
    "    plt.imshow(trainX[idx,:].reshape((28,28)))\n",
    "    plt.title('label is %d'%trainY[idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features is 784\n",
      "number of classes is 10\n",
      "number of training samples is 60000\n",
      "number of testing samples is 10000\n",
      "shape of training data is (60000, 784)\n",
      "shape of training data label is (60000, 784)\n",
      "shape of testing data is (10000, 784)\n",
      "shape of testing data label is (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# normalize the input value to make it between 0 and 1.\n",
    "trainX, testX = trainX/255, testX/255\n",
    "\n",
    "# convert labels to one-hot vector.\n",
    "def to_onehot(y):\n",
    "    y = y.astype(int)\n",
    "    num_class = len(set(y))\n",
    "    Y = np.eye((num_class))\n",
    "    return Y[y]\n",
    "\n",
    "trainY = to_onehot(trainY)\n",
    "testY = to_onehot(testY)\n",
    "num_train, num_feature = trainX.shape\n",
    "num_test, _ = testX.shape\n",
    "_, num_class = trainY.shape\n",
    "print('number of features is %d'%num_feature)\n",
    "print('number of classes is %d'%num_class)\n",
    "print('number of training samples is %d'%num_train)\n",
    "print('number of testing samples is %d'%num_test)\n",
    "print('shape of training data is ' + str(trainX.shape))\n",
    "print('shape of training data label is ' + str(trainX.shape))\n",
    "print('shape of testing data is ' + str(testX.shape))\n",
    "print('shape of testing data label is ' + str(testX.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod, abstractproperty\n",
    "\n",
    "class Activation(ABC):\n",
    "    '''\n",
    "    An abstract class that implements an activation function\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Value of the activation function when input is x.\n",
    "        Parameters:\n",
    "          x is an input to the activation function.\n",
    "        Returns: \n",
    "          Value of the activation function. The shape of the return is the same as that of x.\n",
    "        '''\n",
    "        return x\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Derivative of the activation function with input x.\n",
    "        Parameters:\n",
    "          x is the input to activation function\n",
    "        Returns: \n",
    "          Derivative of the activation function w.r.t x.\n",
    "        '''\n",
    "        return x\n",
    "\n",
    "class Identity(Activation):\n",
    "    '''\n",
    "    Identity activation function. Input and output are identical. \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x\n",
    "    \n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        n, m = x.shape\n",
    "        return np.ones((n, m))\n",
    "    \n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        y = self.value(x)\n",
    "        return y * (1 - y)\n",
    "    \n",
    "class ReLU(Activation):\n",
    "    '''\n",
    "    Rectified linear unit activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        \n",
    "        return 0.005*np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:  \n",
    "        relu = np.where(x >0, 1, 0)#大于0为1，小于等于0为0\n",
    "        return relu\n",
    "\n",
    "\n",
    "class Softmax(Activation):\n",
    "    '''\n",
    "    softmax nonlinear function.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        There are no parameters in softmax function.\n",
    "        '''\n",
    "        super(Softmax, self).__init__()\n",
    "\n",
    "    def value(self, x: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Parameters:\n",
    "          x is the input to the softmax function. x is a two dimensional numpy array. Each row is the input to the softmax function\n",
    "        Returns:\n",
    "          output of the softmax function. The returned value is with the same shape as that of x.\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        # compute the exponential of each element of x\n",
    "        exps = np.exp(x)\n",
    "        # normalize by dividing each element by the sum of all exponentials\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        s = self.value(x)\n",
    "        n, m = x.shape\n",
    "        ds = np.zeros((n, m))\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                for k in range(m):\n",
    "                    if j == k:\n",
    "                        ds[i,j] += s[i,j] * (1 - s[i,j])\n",
    "                    else:\n",
    "                        ds[i,j] -= s[i,j] * s[i,k]\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural network for multi-class classification, the last layer is usually a softmax activation function. The output of the softmax function together with true targets or labels are used to compute the loss function. If both the softmax function and the loss function are computed independently, there are at least one drawbacks:\n",
    "\n",
    "> The derivative of the softmax function w.r.t. the input is a matrix, which is not like the elementwise derivative in ReLU or sigmoid. A batch of such derivatives forms a three dimensional tensor, making the computation complicated.\n",
    "\n",
    "To mitigate such an issue, a common trick is to merge the computation of softmax function with the loss function. In other words, the inputs to the loss function is the input to the softmax other than its output. In deep learning community, often the input to the softmax is regarded as unnormalized probability and is called logits. Being called logits is not exactly correct in math but is widely used. Let $\\boldsymbol{z}$ be the logits, the output of the softmax function $\\hat{\\boldsymbol{y}}$ is defined as\n",
    "\n",
    "$\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}, i = 1, \\cdots, k$\n",
    "\n",
    "the cross entropy loss is computed as follows:\n",
    "\n",
    "$L(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^k y_i\\log(\\hat{y}_i) = \\sum_{i=1}^k y_i\\log\\left(\\frac{e^{z_i}}{\\sum_{j=1}^ke^{z_j}}\\right) = \\sum_{i=1}^k y_i z_i - \\sum_{i=1}^k y_i\\log(\\sum_{j=1}^ke^{z_j})= \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{z_j})$\n",
    "\n",
    "In the above expression, there is a log-sum-exponential term, which often appears in research papers and text book. The reason people prefer the log-sum-exponential term is that it is easy to deal with overflow problem. For instance, there is\n",
    "\n",
    "$L(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{z_j}) = \\sum_{i=1}^k y_i z_i - \\log(\\sum_{j=1}^ke^{\\beta}e^{-\\beta}e^{z_j})= \\sum_{i=1}^k y_i z_i -\\beta - \\log(\\sum_{j=1}^ke^{z_j-\\beta})$\n",
    "\n",
    "By letting $\\beta = \\max z_i$, exponential terms in the exponential expression are all negative, avoiding the overflow problem.\n",
    "\n",
    "The derivative of the above loss function is computed as follows:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z_i} = y_i - \\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$\n",
    ", where the expression $\\frac{e^{z_i}}{\\sum_{j=1}^k e^{z_j}}$ is exactly the $i$th output of the softmax function. Thus,\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\boldsymbol{z}} = \\boldsymbol{y} - \\text{softmax}(\\boldsymbol{z})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# LOSS FUNCTIONS\n",
    "##################################################################################################################\n",
    "\n",
    "class Loss(ABC):\n",
    "    '''\n",
    "    Abstract class for a loss function\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def value(self, yhat: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Value of the empirical loss function.\n",
    "        Parameters:\n",
    "          y_hat is the output of a neural network. The shape of y_hat is (n, k). Each row represents the one sample output.\n",
    "          y contains true labels with shape (n, k).\n",
    "        Returns:\n",
    "          value of the empirical loss function.\n",
    "        '''\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Derivative of the empirical loss function with respect to the predictions.\n",
    "        Parameters:\n",
    "          \n",
    "        Returns:\n",
    "          The derivative of the loss function w.r.t. y_hat. The returned value is a two dimensional array with \n",
    "          shape (n, k)\n",
    "        '''\n",
    "        return yhat\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    '''\n",
    "    Cross entropy loss function\n",
    "    '''\n",
    "\n",
    "    def value(self, yhat: np.ndarray, y: np.ndarray) -> float:\n",
    "        #### write your code below ####\n",
    "        # compute the cross entropy loss for each sample\n",
    "        # use this formula when y and yhat are not one-hot encoded\n",
    "        loss = -np.sum(y * np.log(yhat) + (1 - y) * np.log(1 - yhat), axis=1)\n",
    "        # compute the average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def derivative(self, yhat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        # compute the derivative of cross entropy loss w.r.t. yhat\n",
    "        return -y / yhat + (1 - y) / (1 - yhat)\n",
    "\n",
    "\n",
    "class CEwithLogit(Loss):\n",
    "    '''\n",
    "    Cross entropy loss function with logits (input of softmax activation function) and true labels as inputs.\n",
    "    '''\n",
    "    def value(self, logits: np.ndarray, y: np.ndarray) -> float: \n",
    "        #### write your code below #### \n",
    "        # compute the cross entropy loss using logits and avoiding overflow problem \n",
    "        # use this formula when y is one-hot encoded \n",
    "        beta = np.max(logits) \n",
    "        log_sum_exp = beta + np.log(np.sum(np.exp(logits - beta), axis=1)) \n",
    "        loss = -np.sum(y * logits, axis=1) + log_sum_exp # compute the average loss over all samples \n",
    "        return np.mean(loss)\n",
    "        \n",
    "\n",
    "    def derivative(self, logits: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        #### write your code below ####\n",
    "        # compute the softmax output of logits\n",
    "        yhat = Softmax().value(logits)\n",
    "        # compute the derivative of cross entropy loss and softmax function w.r.t. logits\n",
    "        return yhat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "# METRICS\n",
    "##################################################################################################################\n",
    "\n",
    "def accuracy(y_hat: np.ndarray, y: np.ndarray) -> float:\n",
    "    '''\n",
    "    Accuracy of predictions, given the true labels.\n",
    "    Parameters:\n",
    "      y_hat is the outputs of softmax function. y_hat is with the shape (n, k).\n",
    "      y is the true targets. y is with the shape (n, k).\n",
    "    Returns:\n",
    "      accuracy which is a float number.\n",
    "    '''\n",
    "    # get the predicted class for each sample\n",
    "    y_pred = np.argmax(y_hat, axis=1)\n",
    "    # get the true class for each sample\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    # count the number of correct predictions\n",
    "    num_correct = np.sum(y_pred == y_true)\n",
    "    # calculate the accuracy as the fraction of correct predictions\n",
    "    accuracy = num_correct / y.shape[0]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the python class for neural network.\n",
    "# Using this class, users can design a neural network with any number of layers within which there could be any number of neuros\n",
    "\n",
    "class NeuralNetwork():\n",
    "    '''\n",
    "    Fully connected neural network.\n",
    "    Attributes:\n",
    "      n_layers is the number of layers.\n",
    "      activation is a list of Activation objects corresponding to each layer's activation function.\n",
    "      loss is a Loss object corresponding to the loss function used to train the network.\n",
    "      learning_rate is the learning rate.\n",
    "      W is a list of weight matrix used in each layer.\n",
    "      b is a list of biases used in each layer.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, layer_size: List[int], activation: List[Activation], loss: Loss, learning_rate: float = 0.01) -> None:\n",
    "        '''\n",
    "        Initializes a NeuralNetwork object\n",
    "        '''\n",
    "        assert len(activation) == len(layer_size), \\\n",
    "        \"Number of sizes for layers provided does not equal the number of activation\"\n",
    "        self.layer_size = layer_size\n",
    "        self.num_layer = len(layer_size)\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        for i in range(self.num_layer-1):\n",
    "            W = np.random.randn(layer_size[i], layer_size[i+1]) #/ np.sqrt(layer_size[i])\n",
    "            b = np.random.randn(1, layer_size[i+1])\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "        self.A = []\n",
    "        self.Z = []\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> (List[np.ndarray], List[np.ndarray]):\n",
    "        '''\n",
    "        Forward pass of the network on a dataset of n examples with m features. Except the first layer, each layer\n",
    "        computes linear transformation plus a bias followed by a nonlinear transformation.\n",
    "        Parameters:\n",
    "          X is the training data with shape (n, m).\n",
    "        Returns:\n",
    "          A is a list of numpy data, representing the output of each layer after the first layer. There are \n",
    "            self.num_layer numpy arrays in the list and each array is of shape (n, self.layer_size[i]).\n",
    "          Z is a list of numpy data, representing the input of each layer after the first layer. There are\n",
    "            self.num_layer numpy arrays in the list and each array is of shape (n, self.layer_size[i]).\n",
    "        '''\n",
    "        num_sample = X.shape[0]\n",
    "        A, Z = [], []\n",
    "        #### write your code below ####\n",
    "        # initialize the input of the first layer as X\n",
    "        self.X = X\n",
    "        a = X\n",
    "        # loop over each layer except the last one\n",
    "        for i in range(self.num_layer-1):\n",
    "            # compute the linear transformation z = a * W + b\n",
    "            z = np.dot(a, self.W[i]) + self.b[i]\n",
    "            # compute the nonlinear transformation a = activation(z)\n",
    "            a = self.activation[i].value(z)\n",
    "            # store z and a in Z and A\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "        # return Z and A\n",
    "        self.A = A\n",
    "        self.Z = Z\n",
    "        return Z, A\n",
    "\n",
    "    def backward(self, dLdyhat) -> List[np.ndarray]:\n",
    "        '''\n",
    "        Backward pass of the network on a dataset of n examples with m features. The derivatives are computed from \n",
    "          the end of the network to the front.\n",
    "        Parameters:\n",
    "          dLdyhat is the derivative of the empirical loss w.r.t. yhat which is the output of the neural network.\n",
    "            dLdyhat is with shape (n, self.layer_size[-1])\n",
    "        Returns:\n",
    "          dZ is a list of numpy array. Each numpy array in dZ represents the derivative of the emipirical loss function\n",
    "            w.r.t. the input of that specific layer. There are self.n_layer arrays in the list and each array is of \n",
    "            shape (n, self.layer_size[i])\n",
    "        '''\n",
    "        dZ = []\n",
    "        #### write your code below ####\n",
    "        # initialize the derivative of loss w.r.t. output as dLdyhat\n",
    "        delta = dLdyhat\n",
    "        # loop over each layer from last to first\n",
    "        for i in reversed(range(self.num_layer-1)):\n",
    "            # compute the derivative of loss w.r.t. input as delta * activation'(z)\n",
    "            dz = delta * self.activation[i].derivative(self.Z[i])\n",
    "            # store dz in dZ at the beginning\n",
    "            dZ.insert(0, dz)\n",
    "            # compute the derivative of loss w.r.t. previous output as dz * W.T\n",
    "            delta = np.dot(dz, self.W[i].T)\n",
    "        # return dZ\n",
    "        self.dZ = dZ\n",
    "        return dZ\n",
    "\n",
    "    def update_weights(self) -> List[np.ndarray]:\n",
    "        '''\n",
    "        Having computed the delta values from the backward pass, update each weight with the sum over the training\n",
    "        examples of the gradient of the loss with respect to the weight.\n",
    "        Parameters:\n",
    "          there is no input parameters\n",
    "        Returns:\n",
    "          W is the newly updated weights (i.e. self.W)\n",
    "        '''\n",
    "        #### write your code below ####\n",
    "        # initialize previous output as X\n",
    "        a_prev = self.X\n",
    "        # loop over each layer except last one\n",
    "        for i in range(self.num_layer-1):\n",
    "            # compute gradient of loss w.r.t. weight as a_prev.T * dz / n\n",
    "            dw = np.dot(a_prev.T, self.dZ[i]) / num_sample\n",
    "            # compute gradient of loss w.r.t. bias as sum of dz over axis 0 / n\n",
    "            db = np.sum(self.dZ[i], axis=0, keepdims=True) / num_sample\n",
    "            # update weight as W - learning_rate * dw\n",
    "            self.W[i] -= self.learning_rate * dw\n",
    "            # update bias as b - learning_rate * db\n",
    "            self.b[i] -= self.learning_rate * db\n",
    "            # update previous output as a\n",
    "            a_prev = self.A[i]\n",
    "        # return W\n",
    "        return self.W\n",
    "    \n",
    "    def one_epoch(self, X: np.ndarray,  Y: np.ndarray, batch_size: int, train: bool = True)-> (float, float):\n",
    "        '''\n",
    "        One epoch of either training or testing procedure.\n",
    "        Parameters:\n",
    "          X is the data input. X is a two dimensional numpy array.\n",
    "          Y is the data label. Y is a one dimensional numpy array.\n",
    "          batch_size is the number of samples in each batch.\n",
    "          train is a boolean value indicating training or testing procedure.\n",
    "        Returns:\n",
    "          loss_value is the average loss function value.\n",
    "          acc_value is the prediction accuracy. \n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        slices = list(gen_batches(n, batch_size))\n",
    "        num_batch = len(slices)\n",
    "        idx = list(range(n))\n",
    "        np.random.shuffle(idx)\n",
    "        loss_value, acc_value = 0, 0\n",
    "        for i, index in enumerate(slices):\n",
    "            index = idx[slices[i]]\n",
    "            x, y = X[index,:], Y[index]\n",
    "            Z, A = model.forward(x)   # Execute forward pass\n",
    "            yhat = A[-1]\n",
    "            if train:\n",
    "                dLdz = self.loss.derivative(Z[-1], y)# Calculate derivative of the loss with respect to out\n",
    "                self.backward(dLdz)     # Execute the backward pass to compute the deltas\n",
    "                self.update_weights()  # Calculate the gradients and update the weights\n",
    "            loss_value += self.loss.value(yhat, y)*x.shape[0]\n",
    "            acc_value += accuracy(yhat, y)*x.shape[0]\n",
    "        loss_value = loss_value/n\n",
    "        acc_value = acc_value/n\n",
    "        return loss_value, acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : NeuralNetwork, X: np.ndarray, Y: np.ndarray, batch_size: int, epoches: int) -> (List[np.ndarray], List[float]):\n",
    "    '''\n",
    "    trains the neural network.\n",
    "    Parameters:\n",
    "      model is a NeuralNetwork object.\n",
    "      X is the data input. X is a two dimensional numpy array.\n",
    "      Y is the data label. Y is a one dimensional numpy array.\n",
    "      batch_size is the number of samples in each batch.\n",
    "      epoches is an integer, representing the number of epoches.\n",
    "    Returns:\n",
    "      epoch_loss is a list of float numbers, representing loss function value in all epoches.\n",
    "      epoch_acc is a list of float numbers, representing the accuracies in all epoches.\n",
    "    '''\n",
    "    loss_value, acc = model.one_epoch(X, Y, batch_size, train = False)\n",
    "    epoch_loss, epoch_acc = [loss_value], [acc]\n",
    "    print('Initialization: ', 'loss %.4f  '%loss_value, 'accuracy %.2f'%acc)\n",
    "    for epoch in range(epoches):\n",
    "        if epoch%100 == 0 and epoch > 0: # decrease the learning rate\n",
    "            model.learning_rate = min(model.learning_rate/10, 1.0e-5)\n",
    "        loss_value, acc = model.one_epoch(X, Y, batch_size, train = True)\n",
    "        if epoch%1 == 0:\n",
    "            print(\"Epoch {}/{}: Loss={}, Accuracy={}\".format(epoch, epoches, loss_value, acc))\n",
    "        epoch_loss.append(loss_value)\n",
    "        epoch_acc.append(acc)\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization:  loss 2.3027   accuracy 0.11\n",
      "Epoch 0/2: Loss=2.3022719705439605, Accuracy=0.11846666666666666\n",
      "Epoch 1/2: Loss=2.301647001133583, Accuracy=0.14221666666666666\n",
      "[[ 39 115 191 100  79  32  37 137  49  46]\n",
      " [ 87 142  26  13 113  69  67  46  26 167]\n",
      " [  5  92  27   4  83  13  70  58  25  86]\n",
      " [104 286  40 286  92 167 192  57  91 205]\n",
      " [344 106 190 128 135 175 257  55 281 150]\n",
      " [  1   3   5   0   0   1   2  12   0   2]\n",
      " [ 48 161  92  84  26  64 146  11  70  19]\n",
      " [155 147 112 175 218 237  83 611 205 194]\n",
      " [197  83 349 220 236 134 104  41 227 140]\n",
      " [  0   0   0   0   0   0   0   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.05      0.04       825\n",
      "           1       0.13      0.19      0.15       756\n",
      "           2       0.03      0.06      0.04       463\n",
      "           3       0.28      0.19      0.23      1520\n",
      "           4       0.14      0.07      0.10      1821\n",
      "           5       0.00      0.04      0.00        26\n",
      "           6       0.15      0.20      0.17       721\n",
      "           7       0.59      0.29      0.39      2137\n",
      "           8       0.23      0.13      0.17      1731\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.16     10000\n",
      "   macro avg       0.16      0.12      0.13     10000\n",
      "weighted avg       0.26      0.16      0.19     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# training procedure\n",
    "num_sample, num_feature = trainX.shape\n",
    "epoches = 2\n",
    "batch_size = 512\n",
    "Loss = []\n",
    "Acc = []\n",
    "learning_rate = 1/num_sample*batch_size\n",
    "np.random.seed(2023)\n",
    "model = NeuralNetwork([784, 256, 64, 10], [Identity(), ReLU(), ReLU(), Softmax()], CEwithLogit(), learning_rate = learning_rate)\n",
    "epoch_loss, epoch_acc = train(model, trainX, trainY, batch_size, epoches)\n",
    "# testing procedure\n",
    "test_loss, test_acc = model.one_epoch(testX, testY, batch_size, train = False)\n",
    "z, yhat = model.forward(testX)\n",
    "yhat = yhat[-1]\n",
    "yhat = np.argmax(yhat, axis = 1)\n",
    "y = np.argmax(testY, axis = 1)\n",
    "print(confusion_matrix(yhat, y))\n",
    "print(classification_report(yhat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
